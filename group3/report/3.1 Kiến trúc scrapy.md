## Kiến trúc Scrapy

TODO: update hình

Scrapy Architecture (source: scrapy.org)


### Thành phần

- Scheduler: bộ lập lịch thứ tự các URL download.
- Downloader: thực hiện tải dữ liệu. Quản lý các lỗi khi download. Chống trùng.
- Spiders: bóc tách dữ liệu thành các items và requests
- Item Pipeline: xử lý dữ liệu bóc tách được và lưu vào cơ sở dữ liệu.
- Scrapy Engine: quản lý các thành phần trên.

### Luồng dữ liệu
    Bước 1: Cung cấp URL xuất phát (start_url), được tạo thành một Request lưu trong Scheduler.
    Bước 2 - 3: Scheduler lần lượt lấy các Requests gửi đến Downloader.
    Bước 4 - 5: Downloader tải dữ liệu từ internet, được Responses gửi đến Spiders.
    Bước 6 - 7: Spiders thực hiện:
    •	Bóc tách dữ liệu, thu được Item, gửi đến Item Pipeline.
    •	Tách được URLs, tạo các Requests gửi đến Scheduler.
    Bước 8: Item Pipeline thực hiện xử lý dữ liệu bóc tách được. Đơn giản nhất là thực hiện lưu dữ liệu vào database.
    Bước 9: kiểm tra Scheduler còn Request?
    •	Đúng: quay lại Bước 2.
    •	Sai: kết thúc.

## Cài đặt scrapy

Scrapy chạy trên Python 2.7 và Python 3.3 trở lên 

Cài đặt Scarpy:
- Cách cài đặt qua pip:

        pip install Scrapy
- Cách anaconda:

        conda install -c conda-forge scrapy

## Ví dụ về thiết lập Scrapy:

### Tạo một Scrapy project mới:
    
    scrapy startproject tutorial
    
#### Danh sách mã nguồn:

    ├── __init__.py
    ├── items.py : định nghĩa cấu trúc dữ liệu sẽ bóc tách.
    ├── pipelines.py : định nghĩa hàm thực hiện việc chèn dữ liệu vào database.
    ├── settings.py : cài đặt cấu hình.
    └── spiders
        ├── __init__.py
        └── vietnamnet_vn.py : định nghĩa hàm bóc tách dữ liệu

#### Viết một spider
Spider là class chúng ta định nghĩa và được scrapy sử dụng để scrape thông tin từ một domain (hoặc một nhóm domain)

Chúng ta định nghĩa một danh sách khởi tạo của URLs để download, cách follow links, và cách parse nội dung của pages để trích xuất items.
 
Để tạo một spider, chúng ta tạo một subclass scrapy.Spider và định nghĩa một số thuộc tính:
- name: định danh spider và nó là duy nhất
- start_urls: một danh sách urls cho spider bắt đầu thực hiện crawl. Các trang được download đầu tiên sẽ bắt đầu từ đây, còn lại sẽ được tạo từ dữ liệu đã được lấy về
- parse(): một phương thức sẽ được gọi tới để giải quyết một phản hồi đã được download của mỗi start urls. Phản hồi sẽ được truyền tới phương thức như là tham số đầu tiên và duy nhất của phương thức. Phương thức này có trách nhiệm phân tích dữ liệu phản hồ, trích xuất dữ liệu đã được thu thập, tìm kiếm các url mới và tạo các yêu cầu mới trên các url đó.

#### Chạy spider
Tới thư mục gốc của project và chạy lệnh:

    scrapy crawl spider_name  

Dòng lệnh sẽ gửi một số request tới miền spider_name.toscrape.com

Quá trình thực hiện:
- Scrapy tạo scrapy.Request và gán chúng cho mỗi URL trong danh sách start_urls của spider, phương thức parse được gọi bởi hàm callback.
- Các Request được lập lịch rồi thực thi và trả về đối tượng scrapy.http.Response, sau đó được đưa trở lại spider thông qua phương thức parse().

#### Trích xuất dữ liệu

Cách tốt nhất để trích xuất dữ liệu là thử các selector sử dụng Scrapy shell. Chạy lệnh:
        
        scrapy shell 'http://pider_name.toscrape.com/page/1/'

- Sử dụng cơ chế dựa trên Xpath hoặc biểu thức CSS gọi là Scrapy Selector.

Selector bằng XPath mạnh mẽ hơn CSS

- Scrapy cung cấp class Selector và một số quy ước, shortcut để làm việc với biểu thức xpath và css.
- Đối tượng Selector đại diện các nodes ở trong một văn bản có cấu trúc. Vì thế đầu tiên khởi tạo một selector gắn với node gốc hoặc toàn bộ tài liệu.
- Selector có 4 phương thức cơ bản:

        - xpath(): trả về danh sách các selectors, mỗi cái đại diện cho một node đã được chọn bằng tham số biểu thức xpath truyền vào.
        - css(): trả về danh sách các selectors, mỗi cái đại diện cho một node đã được chọn bằng tham số biểu thức css truyền vào.
        - extract(): trả về một list unicode string với dữ liệu được chọn -> có thể dùng extract_first() để lấy 1 phần tử đầu tiên 
        - re(): trả về danh sách unicode string đã được trích xuất bằng applying tham số biểu thức chính quy truyền vào.

Đối tượng response có thuộc tính selector là thực thể của lớp Selector. Chúng ta có thể truy vấn bằng cách: response.selector.xpath() hoặc response.selector.css() hoặc sử dụng shortcut: response.xpath() hoặc response.css()

##### Sử dụng item:

Có thể truy cập vào các trường bằng cách:

    item = DmozItem() //DmozItem là tên class định nghĩa item
    item['title'] = 'Example title'

Sử dụng item trong phương thức parse() (đối tượng yield Item)
yield là gì : http://phocode.com/python/python-iterator-va-generator/

Các link kéo theo:
        url = response.urljoin(href.extract())
        yield scrapy.Request(url, callback=self.parse_dir_contents)

##### Lưu trữ dữ liệu đã thu thập được.
Cách đơn giản để lưu trữ dữ liệu thu thập được là sử dụng Feed exports, sử dụng câu lệnh:
    
    scrapy crawl spider_name -o quotes.json

