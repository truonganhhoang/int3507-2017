## Cài đặt scrapy

Scrapy chạy trên Python 2.7 và Python 3.3 trở lên 

Cài đặt Scarpy[1]:
- Cách cài đặt qua pip:
```lightning
pip install Scrapy
```
- Cách anaconda:
```lightning
conda install -c conda-forge scrapy
```

## Ví dụ về thiết lập Scrapy:

### Tạo một Scrapy project mới:

```lightning
scrapy startproject tutorial
```

#### Danh sách mã nguồn:
```lightning
├── __init__.py
    ├── items.py : định nghĩa cấu trúc dữ liệu sẽ bóc tách.
    ├── pipelines.py : định nghĩa hàm thực hiện việc chèn dữ liệu vào database.
    ├── settings.py : cài đặt cấu hình.
    └── spiders
        ├── __init__.py
        └── vietnamnet_vn.py : định nghĩa hàm bóc tách dữ liệu

```
#### Viết một spider
[Spider](#spider) là lớp (class) chúng ta định nghĩa và được scrapy sử dụng để cào thông tin từ một miền 
(hoặc một nhóm  miền).

Chúng ta định nghĩa một danh sách khởi tạo của URLs để tải, cách lấy các link kéo theo, và cách phân tích 
cú pháp (parse) nội dung của các trang để trích xuất các mục.

Ví dụ tạo một spider với tên quotes_spider.py tại đường dẫn `scrapy.Spider` :

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)

```
 
Một số thuộc tính được định nghĩa:
- `name`: định danh spider và nó là duy nhất.
- `start_urls`: một danh sách urls cho spider bắt đầu thực hiện cào dữ liệu. Các trang được tải đầu tiên 
sẽ bắt đầu từ đây, còn lại sẽ được tạo từ dữ liệu đã được lấy về.
- `parse()`: một phương thức sẽ được gọi tới để giải quyết một phản hồi đã được tải về của mỗi `start_urls`. 
Phản hồi sẽ được truyền tới phương thức như là tham số đầu tiên và duy nhất. Phương thức này có trách 
nhiệm phân tích dữ liệu phản hồi, trích xuất dữ liệu đã được thu thập, tìm kiếm các url mới và tạo các 
yêu cầu mới trên các url đó.

#### Chạy spider
Tới thư mục gốc của dự án và chạy lệnh:

```lightning
scrapy crawl spider_name
```

Dòng lệnh sẽ gửi một số yêu cầu tới quotes.toscrape.com.

Quá trình thực hiện:
- Scrapy tạo `scrapy.Request` và gán chúng cho mỗi Url trong danh sách `start_urls` của spider, 
phương thức `parse` được gọi bởi hàm `callback`.
- Các yêu cầu (request) được lập lịch rồi thực thi và trả về đối tượng `scrapy.http.Response`, 
sau đó được đưa trở lại spider thông qua phương thức `parse()`.

#### Trích xuất dữ liệu

Cách tốt nhất để trích xuất dữ liệu là dùng các bộ chọn (selector) sử dụng `Scrapy shell`. Chạy lệnh:

```python
scrapy shell 'http://quotes.toscrape.com/page/1/'
```

Sử dụng cơ chế dựa trên Xpath hoặc biểu thức CSS gọi là Scrapy Selector.
Selector (bộ chọn) bằng XPath mạnh mẽ hơn CSS

- Scrapy cung cấp lớp Selector và một số quy ước, cú pháp để làm việc với biểu thức xpath và css.
- Đối tượng selector đại diện các nút (nodes) ở trong một văn bản có cấu trúc. Vì thế đầu tiên khởi 
tạo một selector gắn với nút gốc hoặc toàn bộ tài liệu.
- Selector có 4 phương thức cơ bản:
    - `xpath()`: trả về danh sách các selectors, mỗi cái đại diện cho một nút đã được chọn bằng tham 
    số biểu thức xpath truyền vào.
    - `css()`: trả về danh sách các selectors, mỗi cái đại diện cho một nút đã được chọn bằng tham số 
    biểu thức css truyền vào.
    - `extract()`: trả về một danh sách chuỗi unicode với dữ liệu được chọn -> có thể dùng `extract_first()` 
    để lấy 1 phần tử đầu tiên. 
    - `re()`: trả về danh sách chuỗi unicode đã được trích xuất bằng áp dụng tham số biểu thức chính quy 
    truyền vào.

Đối tượng kết quả (response) có thuộc tính selector là thực thể của lớp Selector. Chúng ta có thể truy 
vấn bằng cách:
 
```lightning
response.selector.xpath() 
```
hoặc 
```lightning
response.selector.css() 
```
hoặc sử dụng tắt (một trong hai): 

```lightning
response.xpath()
response.css()
```

##### Sử dụng item (dữ liệu trả về của một trang):

- Có thể truy cập vào các trường bằng cách:

```lightning
item = DmozItem() //DmozItem là tên lớp định nghĩa item
item['title'] = 'Example title'
```

- Sử dụng item trong phương thức `parse()` (đối tượng `yield` item)

`yield`[2] có chức năng giống như `return` nhưng chỉ khác ở chỗ: Khi hàm 
được gọi lần thứ nhất, nó trả về các phần tử/giá trị tại từ khóa 
`yield` đầu tiên, khi hàm được gọi lần thứ 2, thì hàm lại bắt đầu chạy 
ngay phía sau từ khóa `yield` thứ nhất cho đến khi không gặp từ khóa 
`yield` nào nữa.

- Các link kéo theo bởi link đầu tiên (follow links):

```python
url = response.urljoin(href.extract())
yield scrapy.Request(url, callback=self.parse_dir_contents)
```

##### Lưu trữ dữ liệu đã thu thập được.

Cách đơn giản để lưu trữ dữ liệu thu thập được là sử dụng Feed exports[3], dùng câu lệnh:

```lightning
scrapy crawl spider_name -o quotes.json
```

[1] https://doc.scrapy.org/en/latest/intro/install.html#installing-scrapy

[2] http://phocode.com/python/python-iterator-va-generator/

[3] https://doc.scrapy.org/en/0.16/topics/feed-exports.html#topics-feed-exports