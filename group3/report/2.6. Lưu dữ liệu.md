## 2.6. Lưu dữ liệu vào cơ sở dữ liệu <a name="save-data"></a>

Sau khi dữ liệu được lấy về từ trang web sẽ được lưu trữ vào cơ sở dữ liệu.
Sau đây là mô tả về thành phần pipeline phục vụ cho việc lưu dữ liệu vào
cơ sở dữ liệu MongoDB.
  
#### Cấu trúc Pipeline.py

Dữ liệu thu thập được sẽ được đưa đến item pipeline để bóc tách và ghi vào MongoDB. 
Việc này được thực hiện bởi một class định nghĩa trong tệp pipeline.py như trong ví
dụ sau:

```python
import pymongo
class MongoPipeline(object):
    collection_name = 'scrapy_items'
    
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    
    def close_spider(self, spider):
        self.client.close()
    def process_item(self, item, spider):
            self.db[self.collection_name].insert(dict(item))
            return item
```

#### Xuất/nhập dữ liệu vào MongoDB
##### Xuất dữ liệu từ máy chủ

Sau khi đã hoàn thành bước tách cấu trúc trang, dữ liệu được đẩy vào MongoDB qua pipeline. 
Để xuất dữ liệu từ MongoDB ta thực hiện:

```lightning
mongodump --archive=crawler.`date +%Y-%m-%d"_"%H-%M-%S`.gz --gzip --db crawler
```

##### Nhập dữ liệu vào MongoDB

Sao chép thư mục gzip từ chỗ hiện tại đến thư mục chứa nó bằng docker:

```lightning
docker cp /path/to/file container_id:/root
```

Khôi phục:

```lightning
mongorestore --gzip --archive=/root/crawler.2016-04-18_07-40-11.gz --db crawler
```

[1] : https://hub.docker.com/_/mongo/
