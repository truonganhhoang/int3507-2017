#Spider
Spider là lớp định nghĩa cách cào một hay nhiều trang, bao gồm cách thực hiện thu thập thông tin và trích xuất dữ liệu có cấu trúc. Nói cách khác, Spider là nơi xác định hành vi tùy chỉnh để thu thập dữ liệu và phân tích cú pháp các trang cho một trang web cụ thể.

Chu kỳ cào dữ liệu đối với Spider như sau:
1. Bắt đầu bằng cách tạo ra request (yêu cầu) ban đầu để thu thập thông tin các URL đầu tiên và chỉ định hàm callback để được gọi với các phản hồi đã tải về từ các requests. Các requests đầu tiên được thực hiện bằng cách gọi phương thức `start_request()`, phương thức mặc định này tạo `Request` cho các URL được chỉ định trong phương thức `start_urls` và `parse` như hàm callback
2. Trong hàm callback, phân tích phản hồi và trả về các đối tượng `Item`, `Request`. Những Requests này cũng sẽ bao gồm một callback (có thể giống nhau) và sau đó sẽ được tải xuống bởi Scrapy và sau đó phản hồi sẽ được  xử lý bởi các callback được chỉ định
3. Trong các hàm callback, phân tích nội dung trang, thường dử dụng [Selectors](https://doc.scrapy.org/en/0.16/topics/selectors.html#topics-selectors "Selectors") (cũng có thể dùng BeautifulSoup, lxml, v.v..) và tạo ra các items có dữ liệu đã được phân tích
4. Cuối cùng, các items được trả về từ Spider sẽ được duy trì trong cơ sở dữ liệu (trong một số Item Pipeline) hoặc được ghi vào một tập tin sử dụng [Feed exports](https://doc.scrapy.org/en/0.16/topics/feed-exports.html#topics-feed-exports).

Mặc dù chu kỳ này được áp dụng cho bất kỳ loại Spider nào nhưng có nhiều loại Spider mặc định khác nhau trong Scrapy cho các mục đích khác nhau:

##scrapy.Spider
`class scrapy.spider.Spider`

Đây là spider đơn giản nhất và tất cả các spider khác đều phải kế thừa. Nó không cung cấp bất kì một chức năng đặc biệt nào, chỉ cung cấp `start_menu()` gửi các request từ thuộc tính `start_urls` spider và gọi các phương thức parse của spider cho mỗi kết quả phản hồi.

##Spider arguments

Spider có thể nhận được các đối số (argument) sửa đổi hành vi của chúng. Một số cách sử dụng phổ biến cho đối số spider là xác định URL bắt đầu hoặc để hạn chế thu thập thông tin đến các phần nhất định của trang web nhưng chúng có thể được dùng để cấu hình bất kì chức năng nào của spider

Các đối số spider có thể được truyền thông qua lệnh `crawl` dùng tùy chọn `-a`. Ví dụ:

`scrapy crawl myspider -a category=electronics`

Spider có thể truy cập các đối số trong phương thức *__init__*:
```

 import scrapy
 class MySpider(scrapy.Spider):
    name = 'myspider'
    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
```
Phương thức mặc định *__init__* sẽ lấy bất kỳ đối số spider nào và sao chép chúng vào spider như các thuộc tính. Ví dụ trên cũng có thể được viết như sau:
```

 import scrapy
 class MySpider(scrapy.Spider):
    name = 'myspider'
    def start_requests(self):
        yield scrapy.Request('http://www.example.com/categories/%s' % self.category)
```

##Các spider phổ biến

Scrapy đi kèm với một số spdier phổ biến hữu ích, có thể dùng để phân lớp spider. Mục tiêu của chúng là cung cấp chức năng tiện lợi cho một số trường hợp cào thông thường như theo tất cả các liên kết trên một trang web dựa trên các quy tắc nhất định, thu thập thông tin từ [Sitemaps](https://www.sitemaps.org/index.html) hoặc phân tích nguồn cấp dữ liệu XML/CSV.

### CrawlSpider

`class scrapy.spiders.CrawlSpider`

Đây là spider phổ biến nhất được dùng để thu thập dữ liệu các trang web thông thường. Vì nó cung cấp cơ chế thuận tiện cho việc liên kết sau bằng cách định nghĩa một bộ quy tắc. Nó có thể không phù hợp nhất cho các trang web hoặc dự án cụ thể của người dùng, vì thế người dùng có thể bắt đầu từ CrawlSpider và ghi đè theo theo nhu cầu cho nhiều chức năng tùy chỉnh hoặc chỉ thực hiện spider riêng của mình.

### XMLFeedSpider

`class scrapy.spiders.XMLFeedSpider`

XMLFeedSpider được thiết kế để phân tích các nguồn cung cấp dữ liệu XML bằng cách lặp lại chúng thông qua tên nút nhất định. Trình lặp có thể được chọn từ *iternodes*, *xml* và *html*.

### CSVFeedSpider

`class scrapy.spiders.CSVFeedSpider`

Giống với XMLFeedSpider, CSVFeedSpider lặp qua các hàng thay vì các nút như XMLFeedSpider. Phương thức được gọi trong mỗi lần lặp là `parse_row()`.

### SitemapSpider

`class scrapy.spiders.SitemapSpider`

SitemapSpider cho phép thu thập thông tin trang web bằng cách tìm kiếm các URL sử dụng [Sitemaps](https://www.sitemaps.org/index.html).
Spider này hỗ trợ sơ đồ trang web lồng nhau và tìm kiếm sơ đồ trang web URL từ [robots.txt](http://www.robotstxt.org/).
