## Spider <a name="spider"></a>

Spider[1] là lớp định nghĩa cách cào một hay nhiều trang, bao gồm cách thực hiện thu thập thông tin và 
trích xuất dữ liệu có cấu trúc. Nói cách khác, Spider là nơi xác định hành vi tùy chỉnh để thu thập 
dữ liệu và phân tích cú pháp các trang cho một trang web cụ thể.

Chu kỳ cào dữ liệu đối với spider như sau:
1. Bắt đầu bằng cách tạo ra yêu cầu (request) ban đầu để thu thập thông tin các URL đầu tiên và 
chỉ định hàm `callback` được gọi với tham số là các phản hồi trả về từ các `Request`. 
Các yêu cầu đầu tiên được thực hiện bằng cách gọi phương thức `start_request()`, 
phương thức mặc định này tạo `Request` cho các URL được chỉ định trong `start_urls` 
và phương thức `parse`.

2. Trong hàm callback, phân tích phản hồi và trả về các đối tượng `Item`, `Request`. Những yêu cầu 
này cũng sẽ bao gồm một callback (có thể giống nhau) sẽ được tải xuống bởi scrapy và 
sau đó phản hồi sẽ được  xử lý bởi callback được chỉ định.

3. Trong các hàm callback, phân tích nội dung trang, thường dử dụng Selectors [1] (cũng có thể dùng 
BeautifulSoup, lxml, v.v..) và tạo ra các item có dữ liệu đã được phân tích.

4. Cuối cùng, các item được trả về từ spider sẽ được duy trì trong cơ sở dữ liệu (trong một số 
item pipeline) hoặc được ghi vào một tập tin sử dụng Feed exports.

Mặc dù chu kỳ này được áp dụng cho bất kỳ loại spider nào nhưng có nhiều loại spider mặc định khác nhau 
trong Scrapy cho các mục đích khác nhau. Các loại spider mặc định đó sẽ được chỉ ra sau đây.

### crapy.Spider

```python
class scrapy.spider.Spider
```

Đây là spider đơn giản nhất và tất cả các spider khác đều phải kế thừa. Nó không cung cấp bất kì một 
chức năng đặc biệt nào, chỉ cung cấp `start_menu()` gửi các yêu cầu từ thuộc tính `start_urls` spider 
và gọi các phương thức `parse()` của spider cho mỗi kết quả phản hồi.

### Đối số spider

Spider có thể nhận được các đối số (argument) sửa đổi hành vi của chúng. Một số cách sử dụng phổ biến 
cho đối số spider là xác định URL bắt đầu hoặc để hạn chế thu thập thông tin đến các phần nhất định 
của trang web. Tuy nhiên chúng có thể được dùng để cấu hình bất kì chức năng nào của spider.

Các đối số spider có thể được truyền thông qua lệnh `crawl` dùng tùy chọn `-a`. Ví dụ:

```python
scrapy crawl myspider -a category=electronics
```

Các spider có thể truy cập các đối số trong phương thức `__init__` của chúng:

```python
import scrapy
class MySpider(scrapy.Spider):
    name = 'myspider'
    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
```

Phương thức mặc định `__init__` sẽ lấy bất kỳ đối số spider nào và sao chép chúng vào spider như 
các thuộc tính. Ví dụ trên cũng có thể được viết như sau:

```python
import scrapy
class MySpider(scrapy.Spider):
    name = 'myspider'
    def start_requests(self):
        yield scrapy.Request('http://www.example.com/categories/%s' % self.category)
```

### Các spider phổ biến

Scrapy đi kèm với một số spdier phổ biến hữu ích, có thể dùng để phân lớp spider. Mục tiêu của chúng là 
cung cấp chức năng tiện lợi cho một số trường hợp cào thông thường như lần theo tất cả các liên kết trên 
một trang web dựa trên các quy tắc nhất định, thu thập thông tin từ Sitemaps [2] hoặc phân tích 
nguồn cấp dữ liệu XML/CSV.

#### CrawlSpider

```python
class scrapy.spiders.CrawlSpider
```

Đây là spider phổ biến nhất được dùng để thu thập dữ liệu các trang web thông thường. Vì nó cung cấp 
cơ chế thuận tiện cho việc liên kết sau bằng cách định nghĩa một bộ quy tắc. Nó có thể không phù hợp 
nhất cho các trang web hoặc dự án cụ thể của người dùng, vì thế người dùng có thể bắt đầu từ 
CrawlSpider và ghi đè theo theo nhu cầu cho nhiều chức năng tùy chỉnh hoặc chỉ thực hiện spider riêng 
của mình.

#### XMLFeedSpider

```python
class scrapy.spiders.XMLFeedSpider
```

XMLFeedSpider được thiết kế để phân tích các nguồn cung cấp dữ liệu XML bằng cách lặp lại chúng thông qua 
tên nút nhất định. Trình lặp có thể được chọn từ *iternodes*, *xml* và *html*.

#### CSVFeedSpider

```python
class scrapy.spiders.CSVFeedSpider
```

Giống với XMLFeedSpider, CSVFeedSpider lặp qua các hàng thay vì các nút. 
Phương thức được gọi trong mỗi lần lặp là `parse_row()`.

#### SitemapSpider

```python
class scrapy.spiders.SitemapSpider
```

SitemapSpider cho phép thu thập thông tin trang web bằng cách tìm kiếm các URL sử dụng Sitemaps.
Spider này hỗ trợ sơ đồ trang web lồng nhau và tìm kiếm sơ đồ trang web URL từ robots.txt [3].

[1] https://doc.scrapy.org/en/0.16/topics/selectors.html#topics-selectors "Selectors"

[2] https://www.sitemaps.org/index.html

[3] http://www.robotstxt.org/