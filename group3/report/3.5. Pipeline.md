## Item Pipeline

Một item sau khi đã được cào bởi spider sẽ được chuyển đến Item Pipeline để xử lí thông qua một số 
thành phần được thực hiện tuần tự.

Mỗi thành phần Item pipeline là một lớp Python thực hiện một phương thức đơn giản. Các lớp này nhận 
item và thực hiện các công việc đối với item và đồng thời quyết định xem item đó có tiếp thục pipeline 
hay bị bỏ và không còn được tiến hành nữa.

Các ứng dụng tiêu biểu của Item pipeline:
* Dọn dẹp dữ liệu HTML.
* Xác nhận dữ liệu được cào (kiểm tra các item chứa các trường nhất định).
* Kiểm tra (và bỏ) các bản sao.
* Lưu trữ các item được cào trong cơ sở dữ liệu.

### Viết item pipeline
Mỗi thành phần item pipeline là một lớp Python thực hiện các phương thức:

```python
process_item(item, spider)
```  
Phương thức này được gọi cho mỗi thành phần item pipeline và phải trả về một đối tượng `Item` 
(hoặc bất kỳ lớp con nào) hoặc đưa ra một ngoại lệ `DropItem`. Các item bị loại bỏ không còn được xử lý 
bởi pipeline nữa.

```python
open_spider(spider)
```
Phương thức này được gọi khi spider được mở.

```python
close_spider(spider)
```
Phương thức này được gọi khi spider bị đóng.

### Ví dụ về item pipeline
#### Xác nhận giá cả và bỏ các mặt hàng không có giá

Giả sử điều chỉnh thuộc tính `price` các mặt hàng không bao gồm thuế VAT (thuộc tính `price_excludes_vat`), 
và bỏ các item không có giá:

```python
from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)
```
#### Viết item vào tệp JSON
Pipeline sau đây lưu trữ tất cả các item đã cào (từ tất cả spider) vào một tệp `items.jl`, chứa mỗi item 
trên mỗi dòng, được tuần thự theo định dạng JSON:

```python
import json

class JsonWriterPipeline(object):

    def __init__(self):
        self.file = open('items.jl', 'wb')
    
    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
```

#### Ghi các item vào MongoDB

Ghi các item vào MongoDB bằng cách dùng pymongo [1]. Địa chỉ MongoDB và tên cơ sở dữ liệu được chỉ định 
trong phần cài đặt scrapy.

Điểm chính của ví dụ này là chỉ ra cách sử dụng phương thức `from_crawler()` và cách làm sạch các 
tài nguyên đúng cách:

```python
import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'
    
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    
    def close_spider(self, spider):
        self.client.close()
    
    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
```

[1] https://api.mongodb.com/python/current/