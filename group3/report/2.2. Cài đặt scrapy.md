## 2.2. Cài đặt scrapy <a name="scrapy-setup"></a>

Thư viện Scrapy được viết bằng Python, hỗ trợ cả 2 phiên bản Python 2.7 và Python 3.3 trở lên.
Scrapy hỗ trợ 2 cách cài đặt là sử dụng pip và Anaconda

Cài đặt Scrapy[1]

- Cách 1: sử dụng `pip` là một trình quản lý gói và thư viện cho Python.
Với `pip` việc cài đặt đơn giản bằng 1 dòng lệnh như sau

```lightning
pip install Scrapy
```

- Cách 2: Sử dụng ứng dụng mã nguồn mở Anaconda hỗ trợ cài đặt 
thư viện Python:

```lightning
conda install -c conda-forge scrapy
```

### Ví dụ về thiết lập Scrapy

#### Tạo một Scrapy project mới

Để tạo một Scrapy project mới, ta thực hiện:

```lightning
scrapy startproject tutorial
```

#### Danh sách mã nguồn:
Sau khi chạy lệnh khởi tạo project. Cấu trúc chương trình được trình bày như sau:

```lightning
├── __init__.py
    ├── items.py : định nghĩa cấu trúc dữ liệu sẽ bóc tách.
    ├── pipelines.py : định nghĩa hàm thực hiện việc chèn dữ liệu vào database.
    ├── settings.py : cài đặt cấu hình.
    └── spiders
        ├── __init__.py
        └── vietnamnet_vn.py : định nghĩa hàm bóc tách dữ liệu

```

#### Viết một spider

[Spider](#spider) là lớp được người dùng định nghĩa. Scrapy sử dụng spider này để thu thập 
thông tin từ một miền (hoặc một nhóm  miền).

Trong spider định nghĩa một danh sách các URLs khởi tạo cần thu thập dữ liệu, cách lấy 
các link kéo theo, và cách phân tích cú pháp (parse) nội dung của các trang để 
trích xuất các mục dữ liệu.

Ví dụ tạo một spider với tên quotes_spider.py tại đường dẫn `scrapy.Spider` :

```python
import scrapy


class QuotesSpider(scrapy.Spider):  #extend class scrapy.Spider
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)

```
 
Một số thuộc tính được định nghĩa trong spider:

- `name`: định danh spider và nó là duy nhất.
- `start_urls`: một danh sách urls cho spider bắt đầu thực hiện thu thập dữ liệu. 
Các trang được tải đầu tiên sẽ bắt đầu từ đây, còn lại sẽ được tạo từ dữ liệu 
đã được lấy về.
- `parse`: một phương thức sẽ được gọi để giải quyết kết quả phản hồi trả về từ 
mỗi `start_urls`. 
Phản hồi sẽ được truyền vào phương thức như là tham số đầu tiên và duy nhất. 
Phương thức `parse()` có trách nhiệm phân tích dữ liệu phản hồi, trích xuất dữ 
liệu đã được thu thập, tìm kiếm các url mới và tạo các yêu cầu mới trên các url đó.

#### Chạy spider

Để thực hiện chạy spider, tới thư mục gốc của dự án và sử dụng lệnh:

```lightning
scrapy crawl quotes
```

Dòng lệnh sẽ gửi một số yêu cầu tới quotes.toscrape.com

Quá trình thực hiện:

- Scrapy tạo `scrapy.Request` và gán chúng cho mỗi url trong danh sách `start_urls` 
của spider, phương thức `parse` được gọi bởi hàm `callback`.
- Các yêu cầu (request) được lập lịch rồi thực thi và trả về đối tượng `scrapy.http.Response`, 
sau đó được đưa trở lại spider thông qua phương thức `parse`.

#### Trích xuất dữ liệu

Sử dụng `Scrapy shell` [2] để nghiên cứu dữ liệu được kết xuất mà không cần phải
chạy spider, chạy lệnh như sau:

```lighting
scrapy shell 'http://quotes.toscrape.com/page/1/'
```

Trên Windows, url được đặt trong dấu nháy kép thay vì dấu nháy đơn như ở trên

```lighting
scrapy shell "http://quotes.toscrape.com/page/1/"
```

Sau khi chạy lệnh `scrapy shell` sẽ xuất hiện thông tin như sau:

```lighting
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com/page/1/>
[s]   response   <200 http://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
>>>
```

Sử dụng `shell` có thể chọn các phần tử bằng cách sử dụng Css, Xpath với đối tượng
trả về. Sử dụng cơ chế kết xuất dữ liệu dựa trên Xpath hoặc biểu thức CSS gọi là 
bộ chọn Scrapy (Scrapy Selector). Bộ chọn bằng XPath mạnh mẽ hơn CSS.

- Scrapy cung cấp lớp Selector và một số quy ước, cú pháp để làm việc với biểu thức 
xpath và css.
- Đối tượng selector đại diện các nút (nodes) ở trong một văn bản có cấu trúc. Vì thế đầu tiên khởi 
tạo một selector gắn với nút gốc hoặc toàn bộ tài liệu.
- Selector có 4 phương thức cơ bản:
    - `xpath()`: trả về danh sách các selectors, mỗi cái đại diện cho một nút đã được chọn bằng tham 
    số biểu thức xpath truyền vào.
    - `css()`: trả về danh sách các selectors, mỗi cái đại diện cho một nút đã được chọn bằng tham số 
    biểu thức css truyền vào.
    - `extract()`: trả về một danh sách chuỗi unicode với dữ liệu được chọn -> có thể dùng `extract_first()` 
    để lấy 1 phần tử đầu tiên. 
    - `re()`: trả về danh sách chuỗi unicode đã được trích xuất bằng áp dụng tham số biểu thức chính quy 
    truyền vào.

Đối tượng kết quả (response) có thuộc tính selector là thực thể của lớp Selector. Chúng ta có thể truy 
vấn bằng cách:
 
```lightning
response.selector.xpath('//title')
[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
```

hoặc
 
```lightning
>>> response.selector.css('title')
[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>] 
```

hoặc sử dụng lệnh tắt

```lightning
response.xpath()
response.css()
```

##### Sử dụng item (dữ liệu trả về của một trang):

- Có thể truy cập vào các trường của item bằng cách:

```lightning
item = DmozItem() //DmozItem là tên lớp định nghĩa item
item['title'] = 'Example title'
```

- Các link kéo theo bởi link đầu tiên (follow links):

```python
url = response.urljoin(href.extract())
yield scrapy.Request(url, callback=self.parse_dir_contents)
```

##### Lưu trữ dữ liệu đã thu thập được.

Cách đơn giản để lưu trữ dữ liệu thu thập được là sử dụng Feed exports[3], dùng câu lệnh:

```lightning
scrapy crawl quotes -o quotes.json
```

[1] https://doc.scrapy.org/en/latest/intro/install.html#installing-scrapy

[2] https://doc.scrapy.org/en/latest/topics/shell.html

[3] https://doc.scrapy.org/en/0.16/topics/feed-exports.html#topics-feed-exports
