## Pipeline

Một item sau khi đã được cào bởi spider sẽ được chuyển đến Item Pipeline để xử lí thông qua một số thành phần được thực hiện tuần tự.

Mỗi thành phần item pipeline là một lớp Python thực hiện một phương thức đơn giản. Các lớp này nhận item và thực hiện các hành động đối với item và đồng thời quyết định xem item đó có tiếp thục pipeline hay bị bỏ hay không còn được xử lý.

Các ứng dụng tiêu biểu của item pipeline:
* Dọn dẹp dữ liệu HTML
* Xác nhận dữ liệu được cào (kiểm tra các items chứa các trường nhất định)
* Kiểm tra (và bỏ) các bản sao
* Lưu trữ các item được cào trong cơ sở dữ liệu

### Viết item pipeline
Mỗi thành phần item pipeline là một lớp Python thực hiện các phương thức:

```python
process_item(item, spider)
```  
Phương thức này được gọi cho mỗi thành phần item pipeline và phải trả về một đối tượng `Item` (hoặc bất kỳ lớp con nào) hoặc đưa ra một ngoại lệ `DropItem`. Các item bị loại bỏ không còn được xử lý bởi pipeline nữa.

```python
open_spider(spider)
```
Phương thức này được gọi khi spider được mở

```python
close_spider(spider)
```
Phương thức này được gọi khi spider bị đóng

### Ví dụ về item pipeline
#### Xác nhận giá cả và bỏ các mặt hàng không có giá
Giả sử điều chỉnh thuộc tính `price` các mặt hàng không bao gồm thuế VAT (thuộc tính `price_excludes_vat`), và bỏ các items không có giá:
```python
from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)
```
#### Viết item vào tệp JSON
Pipeline sau đây lưu trữ tất cả các items đã cào (từ tất cả spiders) vào một tệp `items.jl`, chứa mỗi item trên mỗi dòng, được tuần thự theo định dạng JSON:
  ```python
import json

class JsonWriterPipeline(object):

    def __init__(self):
        self.file = open('items.jl', 'wb')
    
    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
  ```
#### Ghi các item vào MongoDB
Ghi các items vào MongoDB bằng cách dùng pymongo [1]. Địa chỉ MongoDB và tên cơ sở dữ liệu được chỉ định trong phần cài đặt scrapy.

Điểm chính của ví dụ này là chỉ ra cách sử dụng phương thức `from_crawler()` và cách làm sạch các tài nguyên đúng cách:
```python
import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'
    
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    
    def close_spider(self, spider):
        self.client.close()
    
    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
```

[1] https://api.mongodb.com/python/current/